# Reducing Loss

## An Iterative Approach

이전 챕터에서는 loss에 대한 개념을 살펴봤습니다. 그렇다면 이번에는 어떻게 ML 모델이 loss를 반복적으로 줄여나갈 수 있는지에 대해 배워봅시다.

반복 학습(Iterative learning)은 흔히 하는 업앤다운 게임과 유사합니다. 여기서 정답이 되는 숫자를 찾는게 곧 최고의 모델이 되는거죠. 우리가 임의로 가중치($w_1$)값을 추측하면 모델 시스템이 이에 따른 loss값이 얼마인지 알려줄겁니다. 그러면 이제 또다른 가중치($w_2$)값을 추측하고, 또 이에 따른 loss값을 전달받죠. 이런식으로 진행해나가면서 가장 최적의 가중치를 찾는 것이 곧 최선의 모델을 찾는 방법입니다.

<img src="https://developers.google.com/machine-learning/crash-course/images/GradientDescentDiagram.svg?hl=ko">

#### 그림 1. 모델 학습에 대한 반복적 접근(Iterative approach) 방식

우리는 이번 ML코스 내내 이러한 접근 방식을 사용할 겁니다. 반복 전략은 대규모 데이터셋에 적합하도록 확장되기 때문에 머신러닝에서 널리 사용됩니다.

모델은 하나 이상의 Feature를 입력으로 받아, 하나의 예측($y'$)을 결과로 반환합니다. 간단히 말해, 하나의 Feature를 받아 하나의 Prediction을 만들어내는 모델은 아래와 같은 형태죠.

$$ y' = b + w_1x_1 $$

위의 식에서 $b$와 $w_1$에는 무엇이 들어가야 할까요? 선형 회귀 문제에 있어서 사실 이는 그다지 중요하지 않은 것으로 나타났습니다. 임의의 값을 선택해도 되지만, 아래와 같은 값을 사용해보겠습니다.

- $b = 0$
- $w_1 = 0$

처음으로 Feature에 대한 가중치 $w_1$을 10이라고 해봅시다. 그러면 아래와 같은 예측이 나옵니다.

$$ y' = 0 + 0 \cdot 10 = 0 $$

위 다이어그램에서 "loss를 계산한다"는 것은 곧 해당 모델이 사용할 손실함수를 통한 계산값입니다. 우리가 만약 Squared loss 함수(제곱 손실함수)를 사용하기로 정했다면, 아래와 같은 두가지 입력을 손실함수가 요구할겁니다.

- $y'$: Feature x에 대한 모델의 예측
- $y$ : Feature x에 대한 실제 label

마지막으로 위 다이어그램에서 "파라미터 갱신값을 계산한다"는 부분을 살펴봅시다. 이 부분에서 ML 시스템이 손실 함수의 값을 검사하고, $b$와 $w_1$에 대한 새로운 값을 생성해냅니다. 지금은 이 신비한 상자가 새로운 값을 고안한 다음 ML 시스템이 모든 레이블에 대한 모든 Feature들을 재평가하여 손실 함수에 대한 새로운 값을 산출해내고, 새로운 매개변수 값을 산출해낸다는 것만 이해하세요. 그리고 학습은 알고리즘이 가능한 최소의 loss값을 갖게끔 하는 모델에 대한 파라미터를 발견해낼 때까지 계속됩니다. 일반적으로, 이는 전체 loss가 변경되지 않거나, 극적으로 느리게 변경될 때까지 반복됩니다. 여기에 도달한 경우, 우리는 모델이 <b>수렴했다(converged)</b>고 말합니다.