# Reducing Loss

## An Iterative Approach

이전 챕터에서는 loss에 대한 개념을 살펴봤습니다. 그렇다면 이번에는 어떻게 ML 모델이 loss를 반복적으로 줄여나갈 수 있는지에 대해 배워봅시다.

반복 학습(Iterative learning)은 흔히 하는 업앤다운 게임과 유사합니다. 여기서 정답이 되는 숫자를 찾는게 곧 최고의 모델이 되는거죠. 우리가 임의로 가중치($w_1$)값을 추측하면 모델 시스템이 이에 따른 loss값이 얼마인지 알려줄겁니다. 그러면 이제 또다른 가중치($w_2$)값을 추측하고, 또 이에 따른 loss값을 전달받죠. 이런식으로 진행해나가면서 가장 최적의 가중치를 찾는 것이 곧 최선의 모델을 찾는 방법입니다.

<img src="https://developers.google.com/machine-learning/crash-course/images/GradientDescentDiagram.svg?hl=ko">

#### 그림 1. 모델 학습에 대한 반복적 접근(Iterative approach) 방식

우리는 이번 ML코스 내내 이러한 접근 방식을 사용할 겁니다. 반복 전략은 대규모 데이터셋에 적합하도록 확장되기 때문에 머신러닝에서 널리 사용됩니다.

모델은 하나 이상의 Feature를 입력으로 받아, 하나의 예측($y'$)을 결과로 반환합니다. 간단히 말해, 하나의 Feature를 받아 하나의 Prediction을 만들어내는 모델은 아래와 같은 형태죠.

$$ y' = b + w_1x_1 $$

위의 식에서 $b$와 $w_1$에는 무엇이 들어가야 할까요? 선형 회귀 문제에 있어서 사실 이는 그다지 중요하지 않은 것으로 나타났습니다. 임의의 값을 선택해도 되지만, 아래와 같은 값을 사용해보겠습니다.

- $b = 0$
- $w_1 = 0$

처음으로 Feature에 대한 가중치 $w_1$을 10이라고 해봅시다. 그러면 아래와 같은 예측이 나옵니다.

$$ y' = 0 + 0 \cdot 10 = 0 $$

위 다이어그램에서 "loss를 계산한다"는 것은 곧 해당 모델이 사용할 손실함수를 통한 계산값입니다. 우리가 만약 Squared loss 함수(제곱 손실함수)를 사용하기로 정했다면, 아래와 같은 두가지 입력을 손실함수가 요구할겁니다.

- $y'$: Feature x에 대한 모델의 예측
- $y$ : Feature x에 대한 실제 label

마지막으로 위 다이어그램에서 "파라미터 갱신값을 계산한다"는 부분을 살펴봅시다. 이 부분에서 ML 시스템이 손실 함수의 값을 검사하고, $b$와 $w_1$에 대한 새로운 값을 생성해냅니다. 지금은 이 신비한 상자가 새로운 값을 고안한 다음 ML 시스템이 모든 레이블에 대한 모든 Feature들을 재평가하여 손실 함수에 대한 새로운 값을 산출해내고, 새로운 매개변수 값을 산출해낸다는 것만 이해하세요. 그리고 학습은 알고리즘이 가능한 최소의 loss값을 갖게끔 하는 모델에 대한 파라미터를 발견해낼 때까지 계속됩니다. 일반적으로, 이는 전체 loss가 변경되지 않거나, 극적으로 느리게 변경될 때까지 반복됩니다. 여기에 도달한 경우, 우리는 모델이 <b>수렴했다(converged)</b>고 말합니다.

> **요약!** : 머신러닝 모델은 임의의 가중치($w$)와 편향($bias$)으로부터 시작하여 최대한 낮은 loss를 갖게될 때까지 반복적으로 가중치와 편향을 조정해나가며 학습합니다.

## Gradient Descent

위쪽에서 ML 시스템이 loss를 줄이기 위해 가중치와 편향을 조정해나간다는 것은 이해했지만, 구체적으로 그것이 어떻게 이루어지는지에 대해서 아직 우리는 다루지 않았습니다.

가능한 모든 $w_1$값에 대한 손실을 계산할 시간과 컴퓨팅 자원이 충분하다고 가정합시다. 우리가 다루는 회귀 문제의 경우 loss와 $w_1$에 대해 그림을 그려보면 이는 항상 볼록합니다. 다시 말해, 이는 항상 그릇과 같은 모양이 되죠.

<img src="https://developers.google.com/machine-learning/crash-course/images/convex.svg?hl=ko">

#### 그림 2. 회귀 문제에 대한 loss vs. 가중치 플롯

이러한 경우에는 항상 유일한 최솟값을 갖게됩닌다. 즉, 어디 한 군데는 정확히 기울기가 0인 지점이 존재하죠. 그리고 그 최솟값이 되는 지점이 손실 함수가 수렴하는 부분이 됩니다.

전체 데이터셋에 대해 생각할 수 있는 모든 $w_1$ 값에 대해 손실함수를 계산하는 것은 수렴하는 지점을 찾는 비효율적인 방법입니다. 그것보다는, 머신러닝에서 매우 널리 사용되는 <b>경사하강법(Gradient Descent)</b>이라는 매커니즘을 배워봅시다.

경사하강법은 처음에 $w_1$에 대한 시작점에서부터 출발합니다. 시작점은 사실 중요하지 않습니다. 그렇기 때문에 많은 알고리즘은 이를 0 또는 랜덤한 숫자로부터 시작합니다. 아래 플롯에서는 0보다 약간 더 큰 값으로부터 출발해보겠습니다.

<img src="https://developers.google.com/machine-learning/crash-course/images/GradientDescentStartingPoint.svg?hl=ko">

#### 그림 3. 경사하강법에 대한 시작점

경사하강법 알고리즘은 시작점에 대한 loss 곡선의 경사를 계산합니다. 그림 3에서, loss의 경사는 곧 곡선의 기울기를 의미하고, 이 기울기가 값을 얼마나 조정해야하는지에 대한 정보가 됩니다. 만약 가중치가 여러 개가 된다면, 기울기는 가중치에 대한 도함수의 벡터가 됩니다.

경사가 하나의 벡터를 이룬다는 점을 기억하세요. 이는 곧 다음의 특징을 갖습니다.

- 방향(direction)
- 크기(magnitude)

경사는 항상 손실 함수 상에서 가장 가파른 증가 방향을 가리킵니다. 경사하강 알고리즘은 loss를 최대한 빨리 줄이기 위해 **음의 기울기 방향**으로 단계를 진행해나갑니다.

<img src="https://developers.google.com/machine-learning/crash-course/images/GradientDescentNegativeGradient.svg?hl=ko" >

#### 그림 4. 경사 하강은 음의 기울기에 의존합니다.

손실 함수 곡선을 따라 다음 지점을 결정하기 위해, 경사 하강 알고리즘은 아래 그림과 같이 시작점에서 기울기 크기의 일부만큼 더합니다.

<img src="https://developers.google.com/machine-learning/crash-course/images/GradientDescentGradientStep.svg?hl=ko" >

#### 그림 5. 기울기 step은 손실 곡선의 다음 지점으로 이동합니다.

경사 하강법은 이러한 과정을 반복해 나가면서 최소값에 점점 더 가까워지는 과정입니다.

> **노트**: 경사하강법을 수행할 때는, 위의 과정을 일반화하여 모든 모델의 파라미터를 *동시에* 조정합니다. 예를 들어, $w_1$과 편향 $b$의 최적값을 찾기 위해 $w_1$과 $b$ 모두에 대한 기울기를 계산하죠. 그 다음 각각의 기울기에 따라 $w_1$와 $b$의 값을 수정합니다. 그 다음 최소 손실에 도달할 때까지 이 단계를 반복해나갑니다.

## Learning Rate

앞서 살핀대로, 경사 벡터는 방향과 크기를 모두 갖습니다. 경사 하강 알고리즘은 기울기에 <b>학습률(Learning rate)</b>라는 스칼라를 곱해서 다음 지점을 결정합니다. 예를 들어, 경사의 크기가 2.5고, 학습률이 0.01이라면, 경사 하강 알고리즘은 이전 지점에서 0.025만큼 떨어진 다음 지점으로 진행합니다.

<b>하이퍼 파라미터(Hyperparameter)</b>는 프로그래머가 직접 머신러닝 알고리즘 상에서 조정하는 값입니다. 대부분의 머신러닝 프로그래머는 이 학습률을 조정하는데에 대부분의 시간을 할애합니다. 

만약 너무 작은 학습률을 선택하게 되면 시간이 너무 오래 걸리죠.

<img src="https://developers.google.com/machine-learning/crash-course/images/LearningRateTooSmall.svg?hl=ko">

#### 그림 6. 학습률이 너무 작은 경우

반대로, 학습률을 너무 크게 정했을 경우, 다음 지점은 저만치 멀리 가버리기 때문에 수렴하기가 어렵습니다.

<img src="https://developers.google.com/machine-learning/crash-course/images/LearningRateTooLarge.svg?hl=ko">

#### 그림 7. 학습률이 너무 큰 경우

모든 회귀 문제에는 <b>골디락스(Goldilocks)</b> 학습률이 존재합니다. 골디락스 값은 손실 함수가 얼마나 평평한지와 관련됩니다. 만약 손실 함수의 기울기가 작다는 것을 알고 있다면, 더 큰 학습률을 안전하게 시도하여 더 빠른 시간 내에 최소값에 도달할 수 있게 됩니다.

<img src="https://developers.google.com/machine-learning/crash-course/images/LearningRateJustRight.svg?hl=ko">

#### 그림 8. 학습률이 적절한 경우

> 학습률에 대해 : 1차원에서 이상적인 학습률은 $\frac{ 1 }{ f(x)'' }$(x에서 f(x)의 2차 도함수의 역)입니다. 2차원 이상에 대한 이상적인 학습률은 Hessian(2차 편도함수의 행렬)의 역입니다. 일반적인 볼록 함수에 대해서는 훨씬 더 복잡합니다.

## Stochastic Gradient Descent (확률적 경사하강법)

경사하강법에서, <b>배치(Batch)</b>란 한 번의 반복에서 경사를 계산하기 위해 사용하는 예시(example)들의 갯수입니다. 지금껏 우리는 이 배치가 전체 데이터셋이 되는 것 처럼 이야기했지만, 구글의 경우 이 "전체 데이터셋"은 말도 안되게 큰 수치입니다. 게다가 구글 데이터는 Feature의 개수도 엄청나게 많죠. 결국, 한 배치가 너무 과대할 수 있습니다. 배치가 너무 크다면 한번의 반복이라고 해도 매우 오랜 시간이 걸릴 수 있습니다.

예시들 사이에서 랜덤하게 샘플링된 대규모 데이터셋에는 중복 데이터가 포함될 수 있습니다. 실제로 배치 크기가 그다면 중복 가능성이 커집니다. 일부 중복성은 노이즈가 심한 경사를 좀 더 부드럽게 만드는데 유용하지만, 너무 과대한 배치에서는 그다지 유용하지 않습니다.

훨씬 적은 계산을 통해 평균적으로 올바른 경사를 얻을 수 있다면 어떨까요? 데이터셋에서 무작위로 예시를 고르는 것은 훨씬 작은 데이터셋으로부터 큰 평균을 예측할 수 있도록 합니다. <b>확률적 경사하강법(SGD)</b>는 이 아이디어를 극단적으로 활용합니다. 한 반복 당 하나의 예시만 사용하는 것이죠.(배치 크기가 1) 충분한 반복이 주어진다면야 SGD는 올바르게 작동하지만, 매우 노이즈가 많다는 문제가 있습니다. "확률적"이라는 말은 각 배치를 구성하는 하나의 예시가 무작위로 선택됨을 나타내죠.

<b>미니배치 확률적 경사 하강법(mini-batch SGD)<b>는 전체 데이터셋에 대한 반복과 SGD 간의 절충안입니다. 미니 배치는 일반적으로 10개에서 1000개 사이의 예시를 가지며, 이는 무작위로 선택됩니다. 미니 배치 SGD는 SGD의 노이즈 양을 줄이지만, 여전히 전체 배치에 대한 반복보다는 효율적으로 동작합니다.

설명을 쉽게 하기 위해서, 우리는 단일 Feature에 대한 경사 하강법만을 언급했습니다. 경사 하강법은 여러 Feature들을 갖는 경우에도 제대로 동작하니 안심하셔도 됩니다.